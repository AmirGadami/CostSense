# ğŸ’¸ CostSense

**CostSense** is a fine-tuned AI system that estimates the prices of consumer items based on textual descriptions. It supports both **OpenAIâ€™s `gpt-4o-mini`** and **Hugging Face LLaMA models fine-tuned with LoRA**, using a Hugging Face dataset and integrated experiment tracking with **Weights & Biases (wandb)**.

---

## ğŸš€ Features

- ğŸ§  Fine-tuned on a **Hugging Face dataset** for realistic item pricing  
- ğŸ”¬ Supports two fine-tuning tracks:
  - **OpenAIâ€™s `gpt-4o-mini`** via OpenAI API
  - **LLaMA models** with **LoRA** using Hugging Face + PEFT  
- ğŸ“Š Built-in **Weights & Biases integration** for experiment tracking  
- ğŸ—ƒï¸ Converts data to OpenAI-compatible JSONL format  
- ğŸ“¦ Modular architecture with `Item` and `Tester` utilities  
- ğŸ“ˆ Model evaluation and comparison across approaches  

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ gpt_model.ipynb              # Interactive testing of base or fine-tuned models
â”œâ”€â”€ fine_tuned_gpt.ipynb         # Fine-tuning pipeline using OpenAI API
â”œâ”€â”€ lora_fine_tuned.ipynb        # Fine-tuning LLaMA models using Hugging Face + LoRA
â”œâ”€â”€ items.py                     # Item class for data structure and prompt formatting
â”œâ”€â”€ loaders.py                   # Data loading from Hugging Face dataset
â”œâ”€â”€ testing.py                   # Performance metrics and result testing
â”œâ”€â”€ environment.yml              # Conda environment definition (name: llms)
â”œâ”€â”€ train.pkl                    # Preprocessed training data
â”œâ”€â”€ test.pkl                     # Preprocessed testing data
â”œâ”€â”€ fine_tune_train.jsonl        # OpenAI-ready training data
â”œâ”€â”€ fine_tune_validation.jsonl   # OpenAI-ready validation data
```

---

## âš™ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/CostSense.git
cd CostSense
```

### 2. Create and Activate Environment

```bash
conda env create -f environment.yml
conda activate llms
```

### 3. Set Environment Variables

Create a `.env` file in the root directory with the following:

```env
HF_TOKEN=your_huggingface_token
OPENAI_API_KEY=your_openai_api_key
WANDB_API_KEY=your_wandb_api_key
```

---

## ğŸ“Š Dataset

CostSense uses a **Hugging Face dataset** containing item descriptions and ground truth prices. It's preprocessed and stored in `train.pkl` and `test.pkl`. You can swap it with your own dataset using the `Item` format.

---

## ğŸ§ª Fine-Tuning & Evaluation

### ğŸ” Option 1: OpenAI GPT Fine-Tuning

- Convert data to JSONL (`fine_tune_gpt.ipynb`)
- Upload and fine-tune `gpt-4o-mini`
- W&B tracking automatically integrated
- Evaluate predictions:

```python
Tester.test(gpt_fine_tuned, test)
```

### ğŸ¦™ Option 2: LLaMA + LoRA (Hugging Face)

- Use `lora_fine_tuned.ipynb` to fine-tune a LLaMA model
- Leverage `PEFT`, `bitsandbytes`, and Hugging Face's trainer
- Log metrics and results to W&B
- Predict and evaluate using the shared interface

---

## ğŸ“¤ Example Prompt

```json
[
  {"role": "system", "content": "You estimate prices of items. Reply only with the price, no explanation"},
  {"role": "user", "content": "A high-resolution 27-inch 4K monitor with adjustable stand and HDMI support."}
]
```

**Expected Output**:
```
Price is $329.00
```

---

## ğŸ§° Tech Stack

- **OpenAI** (`gpt-4o-mini`)
- **Hugging Face Transformers** & **Datasets**
- **LLaMA + LoRA** with **PEFT**
- **Weights & Biases** for tracking and logging
- **Python**, **Jupyter**, **FAISS**, **Gradio**
- **dotenv**, **bitsandbytes**, **transformers**

---

## ğŸ§  Use Cases

- E-commerce product valuation  
- Retail analytics and forecasting  
- AI chatbots that estimate item prices  
- Educational demos of fine-tuning techniques  

---

## ğŸ“¬ Contact

**Amir Ghadami**  
ğŸ“§ [ah.ghadami75@gmail.com](mailto:ah.ghadami75@gmail.com)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/amirhosseinghadami/) | ğŸ¦ [Twitter/X](https://x.com/Amir_ghadamii) | ğŸ’» [GitHub](https://github.com/amirgadami)